{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro til språkmodeller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spørsmål\n",
    "\n",
    "- Hvordan gjør vi det med nøkler? Per nå leser denne fortsatt fra .env-filen da jeg ikke ville pushe de til github.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanker\n",
    "\n",
    "- Gi mer info i prompt om situasjonen rundt spørreundersøkelsen og hva det har blitt spurt om. Spesielt i situasjonen hvor vi øsnker å oppsummere den generelle viben av feedbacken i hver kategori. Var folk fornøyde? Ønsker de tiltak for forbedring?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AzureChatOpenAI\n",
    "### Språkmodellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "# Get environment variables\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "\n",
    "class Llm:\n",
    "    \"\"\"\n",
    "    Class containing the language model.\n",
    "    \"\"\"\n",
    "\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_GPT_4O_MINI\"],\n",
    "        model=os.environ.get(\"OPENAI_MODEL_GPT_4O_MINI\", default=\"gpt-4o-mini\"),\n",
    "        openai_api_key=os.environ[\"OPENAI_API_KEY_4O\"],\n",
    "        openai_api_version=os.environ[\"OPENAI_API_VERSION_4O\"],\n",
    "        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT_4O\"],\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "\n",
    "class LlmRes:\n",
    "    \"\"\"\n",
    "    Class containing the language model.\n",
    "    \"\"\"\n",
    "\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_GPT_3O_MINI\"],\n",
    "        model=os.environ.get(\"OPENAI_MODEL_GPT_3O_MINI\", default=\"o3-mini\"),\n",
    "        openai_api_key=os.environ[\"OPENAI_API_KEY_3O\"],\n",
    "        openai_api_version=os.environ[\"OPENAI_API_VERSION_3O\"],\n",
    "        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT_3O\"],\n",
    "        reasoning_effort=\"high\",\n",
    "        # reasoning_effort: str,\n",
    "        # Constrains effort on reasoning for reasoning models.\n",
    "        # Reasoning models only, like OpenAI o1 and o3-mini.\n",
    "        # Currently supported values are low, medium, and high.\n",
    "        # Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM = Llm()\n",
    "\n",
    "# print(Llm.llm.invoke(\"whats up?\").content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasettet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class FeedbackData:\n",
    "    \"\"\"\n",
    "    Class containing fake datasets by us :)\n",
    "    \"\"\"\n",
    "\n",
    "    # Load a DataFrame with a specific version of a CSV\n",
    "    file_path = \"../files/shuffled_df_LLM101_filtered.csv\"\n",
    "    df_o = pd.read_csv(file_path)\n",
    "\n",
    "    enr_path = \"../files/random_out.csv\"\n",
    "    df = pd.read_csv(enr_path)#.head(20)\n",
    "\n",
    "\n",
    "# print(FeedbackData.df[\"Feedback\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured output\n",
    "### Pydantic-objekter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional\n",
    "\n",
    "\n",
    "class Categorize_search(BaseModel):\n",
    "    \"\"\"Categorization of IT-questionaire feedback.\"\"\"\n",
    "\n",
    "    categories: Literal[\n",
    "        \"Network\",\n",
    "        \"IT Training\",\n",
    "        \"IT Support\",\n",
    "        'Other'\n",
    "    ] = Field(\n",
    "        description=\"Categorize the feedback into the most fitting category. If the categories provided are not a perfect fit, default to 'Other'.\"\n",
    "    )\n",
    "    #if_other: str = Field(\n",
    "        #description=\"If you chose to categorize the feedback as 'Other', return an explanation as to why and what category you think would be the best fit for the feedback.\"\n",
    "    #)\n",
    "\n",
    "\n",
    "class Categorize_bound(BaseModel):\n",
    "    # Begrenser kategoriene modellen kan velge mellom. Den får kun lov til å putte feedbacken i en av de forhåndsbestemte kategoriene.\n",
    "    \"\"\"Categorization of IT-questionaire feedback.\"\"\"\n",
    "\n",
    "    categories: Literal[\n",
    "        \"Cyber security\",\n",
    "        \"IT training\",\n",
    "        \"IT support\",\n",
    "        \"Quality of technology\",\n",
    "        \"Data quality\",\n",
    "        \"Network\",\n",
    "    ] = Field(description=\"Categorize the feedback into the most fitting category.\")\n",
    "    rating: Optional[int] = Field(\n",
    "        description=\"Your certainty of the corectness of the best category on a scale from 1-10\"\n",
    "    )\n",
    "    reason: str = Field(\n",
    "        description=\"Give a short scentence as to why you think this category is the best fit.\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "class Categorize(BaseModel):\n",
    "    \"Categorization of feedback on IT-services.\"\n",
    "\n",
    "    # Thoughts: Forklar tankegangen din.\n",
    "    cat_1: str = Field(description=\"The best fitting general category. Only one.\")\n",
    "    rating: Optional[int] = Field(\n",
    "        description=\"Your certainty of the corectness of the best category on a scale from 1-10\"\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kategorisering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_feedback(feedback_txt : str, struktur, llm_model) -> dict:\n",
    "    # Prompt\n",
    "    task = f\"\"\"\n",
    "    The following feedback is from an internal survey at 'IT and Things Company' where they asked their employees for feedback on their IT-services in general.\n",
    "    Catgorize the feedback: {feedback_txt}.\n",
    "    \"\"\"\n",
    "\n",
    "    # Defining structured output for the model\n",
    "    structured_llm = llm_model.with_structured_output(\n",
    "        struktur, method=\"function_calling\"\n",
    "    )\n",
    "\n",
    "    # Prompt model\n",
    "    response = structured_llm.invoke(task)\n",
    "    # Return the response\n",
    "    return response.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Din tur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## returnerer kategorisert feedback\n",
    "\n",
    "responses = []\n",
    "\n",
    "for f in FeedbackData.df[\"Feedback\"]:\n",
    "    feedback_4O = categorize_feedback(feedback_txt = f, struktur = Categorize_search, llm_model = Llm.llm)\n",
    "    feedback_3O = categorize_feedback(feedback_txt = f, struktur = Categorize_search, llm_model = LlmRes.llm)\n",
    "    responses.append({'Feedback': f, 'Category_4O': feedback_4O['categories'], 'Category_3O': feedback_3O['categories']})\n",
    "    print(responses)\n",
    "\n",
    "    \n",
    "cat_df = pd.DataFrame(responses)\n",
    "print(cat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "others_df = cat_df[(cat_df['Category'] == 'Other') | (cat_df['Category2'] == 'Other')]\n",
    "print(others_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## returnerer kategorisert feedback\n",
    "\n",
    "responses_new_cat = []\n",
    "\n",
    "for f in others_df[\"Feedback\"]:\n",
    "    output_i = categorize_feedback(feedback_txt = f, struktur = Categorize, llm_model = Llm.llm)\n",
    "    responses_new_cat.append({'Feedback': f,'Category': output_i[\"cat_1\"]})\n",
    "\n",
    "new_cat_df = pd.DataFrame(responses_new_cat)\n",
    "print(new_cat_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
