{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro til språkmodeller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spørsmål\n",
    "\n",
    "- Hvordan gjør vi det med nøkler? Per nå leser denne fortsatt fra .env-filen da jeg ikke ville pushe de til github.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanker\n",
    "\n",
    "- Gi mer info i prompt om situasjonen rundt spørreundersøkelsen og hva det har blitt spurt om. Spesielt i situasjonen hvor vi øsnker å oppsummere den generelle viben av feedbacken i hver kategori. Var folk fornøyde? Ønsker de tiltak for forbedring?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AzureChatOpenAI\n",
    "### Språkmodellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureChatOpenAI\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import os\n",
    "\n",
    "\n",
    "# Get environment variables\n",
    "load_dotenv(find_dotenv(), override=True)\n",
    "\n",
    "\n",
    "class Llm:\n",
    "    \"\"\"\n",
    "    Class containing the language model.\n",
    "    \"\"\"\n",
    "\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_GPT_4O_MINI\"],\n",
    "        model=os.environ.get(\"OPENAI_MODEL_GPT_4O_MINI\", default=\"gpt-4o-mini\"),\n",
    "        openai_api_key=os.environ[\"OPENAI_API_KEY_4O\"],\n",
    "        openai_api_version=os.environ[\"OPENAI_API_VERSION_4O\"],\n",
    "        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT_4O\"],\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "\n",
    "class LlmRes:\n",
    "    \"\"\"\n",
    "    Class containing the language model.\n",
    "    \"\"\"\n",
    "\n",
    "    llm = AzureChatOpenAI(\n",
    "        azure_deployment=os.environ[\"AZURE_OPENAI_DEPLOYMENT_GPT_3O_MINI\"],\n",
    "        model=os.environ.get(\"OPENAI_MODEL_GPT_3O_MINI\", default=\"o3-mini\"),\n",
    "        openai_api_key=os.environ[\"OPENAI_API_KEY_3O\"],\n",
    "        openai_api_version=os.environ[\"OPENAI_API_VERSION_3O\"],\n",
    "        azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT_3O\"],\n",
    "        reasoning_effort=\"high\",\n",
    "        # reasoning_effort: str,\n",
    "        # Constrains effort on reasoning for reasoning models.\n",
    "        # Reasoning models only, like OpenAI o1 and o3-mini.\n",
    "        # Currently supported values are low, medium, and high.\n",
    "        # Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response.\n",
    "    )\n",
    "\n",
    "\n",
    "# LLM = Llm()\n",
    "\n",
    "# print(Llm.llm.invoke(\"whats up?\").content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasettet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class FeedbackData:\n",
    "    \"\"\"\n",
    "    Class containing fake datasets by us :)\n",
    "    \"\"\"\n",
    "\n",
    "    # Load a DataFrame with a specific version of a CSV\n",
    "    file_path = \"../files/shuffled_df_LLM101_filtered.csv\"\n",
    "    df_o = pd.read_csv(file_path)\n",
    "\n",
    "    enr_path = \"../files/output_df.csv\"\n",
    "    df = pd.read_csv(enr_path)\n",
    "\n",
    "\n",
    "# print(FeedbackData.df[\"Feedback\"][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structured output\n",
    "### Pydantic-objekter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal, Optional\n",
    "\n",
    "\n",
    "class Categorize_search(BaseModel):\n",
    "    \"\"\"Categorization of IT-questionaire feedback.\"\"\"\n",
    "\n",
    "    fb: str = Field(description=\"Return just the feedback verbatim.\")\n",
    "    categories: Literal[\n",
    "        \"Cyber security\",\n",
    "        \"IT Support\",\n",
    "        \"IT Systems\",\n",
    "        'Other'\n",
    "    ] = Field(\n",
    "        description=\"Categorize the feedback into the most fitting category. If the categories provided are not a perfect fit, default to 'Other'. You should always choose 'Other' if you belive there exist a category which is a better fit for the feedback than the pedetermined categories in the list.\"\n",
    "    )\n",
    "    #if_other: str = Field(\n",
    "        #description=\"If you chose to categorize the feedback as 'Other', return an explanation as to why and what category you think would be the best fit for the feedback.\"\n",
    "    #)\n",
    "\n",
    "\n",
    "class Categorize_bound(BaseModel):\n",
    "    # Begrenser kategoriene modellen kan velge mellom. Den får kun lov til å putte feedbacken i en av de forhåndsbestemte kategoriene.\n",
    "    \"\"\"Categorization of IT-questionaire feedback.\"\"\"\n",
    "\n",
    "    fb: str = Field(description=\"Return just the feedback verbatim.\")\n",
    "    categories: Literal[\n",
    "        \"Cyber security\",\n",
    "        \"IT training\",\n",
    "        \"IT support\",\n",
    "        \"Quality of technology\",\n",
    "        \"Data quality\",\n",
    "        \"Network\",\n",
    "    ] = Field(description=\"Categorize the feedback into the most fitting category.\")\n",
    "    rating: Optional[int] = Field(\n",
    "        description=\"Your certainty of the corectness of the best category on a scale from 1-10\"\n",
    "    )\n",
    "    reason: str = Field(\n",
    "        description=\"Give a short scentence as to why you think this category is the best fit.\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Categorize_think(BaseModel):\n",
    "    \"\"\"Categorization of IT-questionaire feedback.\"\"\"\n",
    "\n",
    "    cot: str = Field(\n",
    "        description=\"Think about and explain why this category is the most fitting one and why you chose it.\"\n",
    "    )\n",
    "    categories: Literal[\n",
    "        \"Cyber secyrity\", \"IT training\", \"IT support\", \"Technology quality\"\n",
    "    ] = Field(description=\"Categorize the feedback into the most fitting category.\")\n",
    "\n",
    "\n",
    "class Categorize(BaseModel):\n",
    "    \"Categorization of feedback on IT-services.\"\n",
    "\n",
    "    # Thoughts: Forklar tankegangen din.\n",
    "    fb: str = Field(description=\"Return just the feedback verbatim.\")\n",
    "    cat_1: str = Field(description=\"The best fitting general category.\")\n",
    "    rating: Optional[int] = Field(\n",
    "        description=\"Your certainty of the corectness of the best category on a scale from 1-10\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Enrich(BaseModel):\n",
    "    \"Enrichment of dataset to make it more human\"\n",
    "\n",
    "    cat_1: str = Field(\n",
    "        description=\"Return the enriched dataset in a comma separated format like the format of the input dataset\"\n",
    "    )\n",
    "\n",
    "\n",
    "class Sentiment(BaseModel):\n",
    "    \"Provide a sentiment analysis of feedback\"\n",
    "\n",
    "    fb: str = Field(description=\"Return just the feedback verbatim.\")\n",
    "    sentiment: str = Field(\n",
    "        description=\"Return the sentiment analysis of the feedback as a label. Either positive, negative or mixed.\"\n",
    "    )\n",
    "    depth_sent: str = Field(\n",
    "        description=\"Return a more nuanced analysis of the sentiment. Is there more complexity to the sentiment than just positive or negative?\"\n",
    "    )\n",
    "\n",
    "    feeling: str = Field(\n",
    "        description=\"What feelings are most predominant in the sentiment of the feedback? Return just the feeling(s).\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kategorisering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_feedback(data, struktur, llm_model):\n",
    "    # Prompt\n",
    "    task = f\"\"\"\n",
    "    The following feedback is from an internal survey at 'IT and Things Company' where they asked their employees for feedback on their IT-services in general.\n",
    "    Catgorize the feedback: {data}.\n",
    "    \"\"\"\n",
    "\n",
    "    # Giving the task to LLM\n",
    "    structured_llm = llm_model.with_structured_output(\n",
    "        struktur, method=\"function_calling\"\n",
    "    )\n",
    "\n",
    "    response = structured_llm.invoke(task)\n",
    "\n",
    "    # Return the response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Kun forhåndsbestemte kategorier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bound categorization\n",
    "\n",
    "responses_3O = {}\n",
    "responses_4O = {}\n",
    "\n",
    "for i in range(n):\n",
    "    responses_3O[str(i)] = categorize_feedback(\n",
    "        FeedbackData.df[\"Feedback\"][i], Categorize_bound, LlmRes.llm\n",
    "    )\n",
    "    responses_4O[str(i)] = categorize_feedback(FeedbackData.df[\"Feedback\"][i], Categorize_bound, Llm.llm)\n",
    "    print(f\"\\n-----Feedback-----\\n {responses_3O[str(i)].fb}\\n\")\n",
    "    print(\n",
    "        f\"-----Category-----\\n 4O: {responses_4O[str(i)].categories}\\n 3O: {responses_3O[str(i)].categories}\\n\"\n",
    "    )\n",
    "    print(\n",
    "        f\"-----Rating of certainty-----\\n 4O: {responses_4O[str(i)].rating}\\n 3O: {responses_3O[str(i)].rating}\\n\"\n",
    "    )\n",
    "    print(\n",
    "        f\"-----Reason-----\\n 4O: {responses_4O[str(i)].reason}\\n 3O: {responses_3O[str(i)].reason}\\n\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Modellen velger fritt kategorier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "responses_3O = {}\n",
    "responses_4O = {}\n",
    "\n",
    "for i in range(n):\n",
    "    responses_3O[str(i)] = categorize_feedback(\n",
    "        FeedbackData.df[\"Feedback\"][i], Categorize ,LlmRes.llm\n",
    "    )\n",
    "    responses_4O[str(i)] = categorize_feedback(FeedbackData.df[\"Feedback\"][i], Categorize, Llm.llm)\n",
    "    print(f\"\\n-----Feedback-----\\n {responses_3O[str(i)].fb}\\n\")\n",
    "    print(\n",
    "        f\"-----Category 1-----\\n 4O: {responses_4O[str(i)].cat_1}\\n 3O: {responses_3O[str(i)].cat_1}\\n\"\n",
    "    )\n",
    "    print(\n",
    "        f\"-----Rating of certainty-----\\n 4O: {responses_4O[str(i)].rating}\\n 3O: {responses_3O[str(i)].rating}\\n\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Gi noen kategorier + 'other' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "responses_3O = {}\n",
    "responses_4O = {}\n",
    "\n",
    "for i in range(n):\n",
    "    responses_3O[i] = categorize_feedback(FeedbackData.df[\"Feedback\"][i], Categorize_search, LlmRes.llm)\n",
    "    responses_4O[i] = categorize_feedback(FeedbackData.df[\"Feedback\"][i], Categorize_search, Llm.llm)\n",
    "    print(f\"\\n-----Feedback-----\\n {responses_3O[i].fb}\\n\")\n",
    "    print(\n",
    "        f\"-----Category-----\\n 4O: {responses_4O[i].categories}\\n 3O: {responses_3O[i].categories}\\n\"\n",
    "    )\n",
    "    print(\n",
    "        f\"-----If other-----\\n 4O: {responses_4O[i].if_other}\\n 3O: {responses_3O[i].if_other}\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Din tur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Feedback    Category\n",
      "0  Although the system is old and its documentati...  IT systems\n",
      "1  The step-by-step training guides for warehouse...       Other\n",
      "2  Training sessions on inventory management soft...       Other\n",
      "3  The VPN connection is unreliable and slows dow...  IT systems\n",
      "4  Whenever I need help, I receive a prompt and f...  IT Support\n",
      "5  The email filtering system does a great job ov...  IT systems\n",
      "6  The HR software training felt rushed and the s...       Other\n",
      "7  The IT onboarding training was comprehensive a...  IT systems\n",
      "8  The sales analytics platform is overly complex...  IT systems\n",
      "9  Support has been consistently helpful with pro...  IT Support\n"
     ]
    }
   ],
   "source": [
    "## returnerer kategorisert feedback\n",
    "\n",
    "responses_4O = []\n",
    "\n",
    "for i in range(n):\n",
    "    output_i = categorize_feedback(FeedbackData.df[\"Feedback\"][i], Categorize_search, Llm.llm)\n",
    "    responses_4O.append({'Feedback':output_i.fb, 'Category': output_i.categories})\n",
    "    \n",
    "cat_df = pd.DataFrame(responses_4O)\n",
    "print(cat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            Feedback Category\n",
      "1  The step-by-step training guides for warehouse...    Other\n",
      "2  Training sessions on inventory management soft...    Other\n",
      "6  The HR software training felt rushed and the s...    Other\n"
     ]
    }
   ],
   "source": [
    "others_df = cat_df[cat_df['Category']=='Other']\n",
    "print(others_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Category\n",
      "0  Training and Documentation\n",
      "1        Training and Support\n",
      "2        Training and Support\n"
     ]
    }
   ],
   "source": [
    "## returnerer kategorisert feedback\n",
    "\n",
    "responses_new_cat = []\n",
    "\n",
    "for i in others_df[\"Feedback\"]:\n",
    "    output_i = categorize_feedback(i, Categorize, Llm.llm)\n",
    "    responses_new_cat.append({'Category': output_i.cat_1})\n",
    "\n",
    "new_cat_df = pd.DataFrame(responses_new_cat)\n",
    "print(new_cat_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
